<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Haibin Huang</title>
  <link rel="stylesheet" href="style.css"/>
</head>
<body>
  <div class="container">
    <header class="header">
      <img src="assets/profile.jpg" alt="Yuxin Zhang" class="profile-pic">
      <div class="info">
        <h1>Yuxin Zhang</h1>
        <p class="title">Researcher of Generative AI</p>
        <p><a href="#">ByteDance</a></p>
        <p>yuxin.zhazel at gmail dot com</p>
        <p>
          <!-- <a href="#">CV</a> | -->
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=8VD0_DkAAAAJ">Google Scholar</a> |
          <a href="https://github.com/zyxElsa">GitHub</a>
        </p>
      </div>
    </header>

    <section class="about">
      <h2>About Me</h2>
      <p>I recently joined <strong>ByteDance</strong> as Researcher of Generative AI. Previously I received my Ph.D. in computer science from the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, advised by Prof. Changsheng Xu and Prof. Weiming Dong. I received my B.S. in Automation from Tsinghua University in 2020.</p>
      <p>My research interest is in the intersection of Computer Vision and Computer Graphics. I am interested in building interactive, interpretable, and creative generative models. I am currently working on Generative AI, Multimedia, and Computational Art.</p>
        

      <!-- <p>My research is dedicated to the analysis and creation of visual content through the integration of computer vision, computer graphics, and machine learning. I aim to devise techniques that simplify the creation and manipulation of digital content, thereby making these processes more user-friendly and accessible.</p> -->
      <p class="highlight">[2025/07 Update] Our team is hiring full-time research scientists and interns specializing in generative AI, with a focus on Multi-Modal Understanding and Generation (based on Shanghai and Beijing).</p>
    </section>

    <section class="news">
        <h2>News</h2>
        <ul class="news-list">
          <li><strong>2025.01</strong> ðŸŽ‰ðŸŽ‰ Our latest work <em>IP-Prompter: Training-Free Theme-Specific Image Generation via Dynamic Visual Prompting</em> has been accepted by <strong>ACM SIGGRAPH 2025</strong>. <a href="#">Code</a></li>
          <li><strong>2025.05</strong> ðŸŽ‰ðŸŽ‰ <em>MotionCrafter: Plug-and-Play Motion Guidance for Diffusion Models</em> has been accepted by <strong>IEEE TVCG</strong>. <a href="#">Code</a></li>
          <li><strong>2025.04</strong> ðŸŽ‰ðŸŽ‰ <em>B4M: Breaking Low-Rank Adapter for Making Content-Style Customization</em> has been accepted by <strong> ACM TOG</strong>. <a href="#">Code</a></li>
          <li><strong>2023.06</strong> ðŸŽ‰ðŸŽ‰ <em>ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models</em> has been accepted by <strong>ACM SIGGRAPH Asia 2023 / ACM TOG</strong>. <a href="#">Code</a></li>
          <li><strong>2023.05</strong> ðŸŽ‰ðŸŽ‰ <em>A Unified Arbitrary Style Transfer Framework via Adaptive Contrastive Learning</em> has been accepted by <strong>ACM TOG</strong>. <a href="#">Code</a></li>
          <li><strong>2023.04</strong> ðŸŽ‰ðŸŽ‰ <em>Inversion-Based Style Transfer With Diffusion Models</em> has been accepted by <strong>IEEE/CVF CVPR 2023</strong>. <a href="#">Code</a></li>
          <li><strong>2022.12</strong> ðŸŽ‰ðŸŽ‰ <em>Portrait Map Art Generation by Asymmetric Image-to-Image Translation</em> has been accepted by <strong>Leonardo</strong>.</li>
          <li><strong>2022.04</strong> ðŸŽ‰ðŸŽ‰ <em>Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning</em> has been accepted by <strong>ACM SIGGRAPH 2022</strong>. <a href="#">Code</a></li>
        </ul>
    </section>

    <section class="experience">
      <h2>Work Experience</h2>
      <ul>
        <li><strong>ByteDance</strong> â€“ Researcher of Generative AI (July 2025 â€“ now)</li>
      </ul>
    </section>

    <section class="publications">
        <h2>Publications</h2>
      
        <div class="pub">
          <img src="assets/pub1.jpg" alt="Publication 1"/>
          <div>
            <p><strong>Inversion-Based Style Transfer With Diffusion Models</strong><br/>
            Y Zhang, N Huang, F Tang, H Huang, C Ma, W Dong, C Xu<br/>
            <em>CVPR 2023, pp. 10146â€“10156</em> Â· 362 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub2.jpg" alt="Publication 2"/>
          <div>
            <p><strong>Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning</strong><br/>
            Y Zhang, F Tang, W Dong, H Huang, C Ma, TY Lee, C Xu<br/>
            <em>SIGGRAPH 2022, 12:1â€“12:8</em> Â· 206 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub3.jpg" alt="Publication 3"/>
          <div>
            <p><strong>Prospect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models</strong><br/>
            Y Zhang, W Dong, F Tang, N Huang, H Huang, C Ma, TY Lee, O Deussen, ...<br/>
            <em>TOG 42(6), 1â€“14, 2023</em> Â· 103* citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub4.jpg" alt="Publication 4"/>
          <div>
            <p><strong>DiffStyler: Controllable Dual Diffusion for Text-Driven Image Stylization</strong><br/>
            N Huang, Y Zhang, F Tang, C Ma, H Huang, W Dong, C Xu<br/>
            <em>TNNLS 2024</em> Â· 63 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub5.jpg" alt="Publication 5"/>
          <div>
            <p><strong>A Unified Arbitrary Style Transfer Framework via Adaptive Contrastive Learning</strong><br/>
            Y Zhang, F Tang, W Dong, H Huang, C Ma, TY Lee, C Xu<br/>
            <em>TOG 42(5), 1â€“16, 2023</em> Â· 43 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub6.jpg" alt="Publication 6"/>
          <div>
            <p><strong>MotionCrafter: Plug-and-Play Motion Guidance for Diffusion Models</strong><br/>
                Y Zhang, W Dong, F Tang, N Huang, H Huang, C Ma, C Xu<br/>
            <em>IEEE Transactions on Visualization and Computer Graphics</em> Â· 21 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub7.jpg" alt="Publication 7"/>
          <div>
            <p><strong>Style-A-Video: Agile Diffusion for Arbitrary Text-Based Video Style Transfer</strong><br/>
            N Huang, Y Zhang, W Dong<br/>
            <em>IEEE SPL 2024, 31:1494â€“1498</em> Â· 20 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub8.jpg" alt="Publication 8"/>
          <div>
            <p><strong>Music Style Transfer with Time-Varying Inversion of Diffusion Models</strong><br/>
            S Li, Y Zhang, F Tang, C Ma, W Dong, C Xu<br/>
            <em>AAAI 2024, 38(1): 547â€“555</em> Â· 13 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub9.jpg" alt="Publication 9"/>
          <div>
            <p><strong>CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis With Multimodal Diffusion</strong><br/>
            N Huang, W Dong, Y Zhang, F Tang, R Li, C Ma, X Li, TY Lee, C Xu<br/>
            <em>IEEE TVCG 2025</em> Â· 10* citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub10.jpg" alt="Publication 10"/>
          <div>
            <p><strong>HeadRouter: A Training-Free Image Editing Framework for MM-DiTs by Adaptively Routing Attention Heads</strong><br/>
            Y Xu, F Tang, J Cao, Y Zhang, X Kong, J Li, O Deussen, TY Lee<br/>
            <em>arXiv:2411.15034</em> Â· 10 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub11.jpg" alt="Publication 11"/>
          <div>
            <p><strong>B4M: Breaking Low-Rank Adapter for Making Content-Style Customization</strong><br/>
            Y Xu, F Tang, J Cao, Y Zhang, O Deussen, W Dong, J Li, TY Lee<br/>
            <em>TOG 44(2), 1â€“17, 2025</em> Â· 6* citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub12.jpg" alt="Publication 12"/>
          <div>
            <p><strong>Dance-to-Music Generation with Encoder-Based Textual Inversion</strong><br/>
            S Li, W Dong, Y Zhang, F Tang, C Ma, O Deussen, TY Lee, C Xu<br/>
            <em>SIGGRAPH Asia 2024</em> Â· 6 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub13.jpg" alt="Publication 13"/>
          <div>
            <p><strong>LumiSculpt: A Consistency Lighting Control Network for Video Generation</strong><br/>
            Y Zhang, D Zheng, B Gong, J Chen, M Yang, W Dong, C Xu<br/>
            <em>arXiv:2410.22979, 2024</em> Â· 3 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub14.jpg" alt="Publication 14"/>
          <div>
            <p><strong>Portrait Map Art Generation by Asymmetric Image-to-Image Translation</strong><br/>
            Y Zhang, F Tang, W Dong, TNH Le, C Xu, TY Lee<br/>
            <em>Leonardo 56(1), 28â€“36, 2023</em> Â· 3 citations</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub15.jpg" alt="Publication 15"/>
          <div>
            <p><strong>Bringing Characters to New Stories: Training-Free Theme-Specific Image Generation via Dynamic Visual Prompting</strong><br/>
            Y Zhang, M Luo, W Dong, X Yang, H Huang, C Ma, O Deussen, TY Lee, ...<br/>
            <em>arXiv:2501.15641, 2025</em> Â· 1 citation</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub16.jpg" alt="Publication 16"/>
          <div>
            <p><strong>A Comprehensive Evaluation of Arbitrary Image Style Transfer Methods</strong><br/>
            Z Zhou, F Tang, Y Zhang, O Deussen, J Cao, W Dong, X Li, TY Lee<br/>
            <em>IEEE TVCG, 2024</em> Â· 1 citation</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub17.jpg" alt="Publication 17"/>
          <div>
            <p><strong>Quantification of Artist Representativity within an Art Movement</strong><br/>
            Y Zhang, F Tang, W Dong, C Xu<br/>
            <em>ICMEW 2022, pp. 1â€“6</em> Â· 1 citation</p>
          </div>
        </div>
      
        <div class="pub">
          <img src="assets/pub18.jpg" alt="Publication 18"/>
          <div>
            <p><strong>Dance Montage through Style Transfer and Music Generation</strong><br/>
            M Luo, Y Zhang, P Xu, T Wang, Y Bo, X Jin, W Dong<br/>
            <em>SIGGRAPH Asia 2024 Art Papers</em></p>
          </div>
        </div>
    </section>
      
  </div>
</body>
</html>
